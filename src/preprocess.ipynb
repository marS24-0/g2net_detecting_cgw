{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from augmentation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_viterbi(v):\n",
    "    \"\"\"\n",
    "    Extract features (row sums, hough and lbp) from Viterbi transformation\n",
    "    \"\"\"\n",
    "    return (v.sum(axis=1), extract_lines(v), viterbi2lbp(v))\n",
    "\n",
    "def features(spec):\n",
    "    \"\"\"\n",
    "    Handles input dataframe to extract features\n",
    "    \"\"\"\n",
    "    return [features_viterbi(v) for v in viterbi_2w(spec)]\n",
    "\n",
    "def augmented_features(record, df):\n",
    "    spec = spectrogram_normalization(df)\n",
    "    return [\n",
    "        features(spec.copy())\n",
    "    ] + [\n",
    "        features(augment(spec.copy())) for i in range(10-1)\n",
    "    ]\n",
    "\n",
    "def sums(df):\n",
    "    \"\"\"\n",
    "    Extract sum features (peak ratio, area ratio) from data row sums\n",
    "    \"\"\"\n",
    "    return pd.DataFrame([[peaks(somma,5,1), areas(somma,5,1)] for somma in df.values], index=df.index, columns=['p1', 'p2'])\n",
    "\n",
    "def raw2df(data, prefix, train:bool):\n",
    "    \"\"\"\n",
    "    Converts the utility-returned objects to dataframes.\n",
    "    Only handles 1-way (Viterbi transformation on forward or backward spectrogram) at once.\n",
    "\n",
    "    df: dataframe containing sum, and hough features\n",
    "    df_lbp: dataframe containing lbp features\n",
    "    \"\"\"\n",
    "    idx = [x[0] for x in data]\n",
    "    df_sums = pd.DataFrame([x[1][0] for x in data], index=idx)\n",
    "    # df_describe = df_sums.T.describe().T.rename(lambda x:f'{prefix}_{x}', axis=1)\n",
    "    df_sums = sums(df_sums).rename(lambda x:f'{prefix}_{x}', axis=1)\n",
    "    df_hough = pd.DataFrame([x[1][1] for x in data], index=idx).rename(lambda x:f'{prefix}_{x}', axis=1)\n",
    "    df_lbp = pd.DataFrame([x[1][2] for x in data], index=idx) + (1e-9)\n",
    "    df_lbp = df_lbp.rename(lambda x:f'{prefix}_{x}', axis=1)\n",
    "    df = pd.concat((df_sums, df_hough), axis=1)\n",
    "    if train:\n",
    "        for tdf in [df, df_lbp]:\n",
    "            tdf['augm_idx'] = list(range(10))*(tdf.shape[0]//10)\n",
    "            tdf.set_index([tdf.index, 'augm_idx'], inplace=True)\n",
    "            tdf.index.set_names(['record','augm_idx'], inplace=True)\n",
    "    return df, df_lbp\n",
    "\n",
    "def features_mixed(df_fw, df_bw, data_fw, data_bw):\n",
    "    \"\"\"\n",
    "    Compute mixed features from both ways transformations\n",
    "    \"\"\"\n",
    "    p1s = []\n",
    "    p2s = []\n",
    "    pam = []\n",
    "    for x,y in zip(data_fw, data_bw):\n",
    "        xx = x[1][0]\n",
    "        yy = y[1][0]\n",
    "        pam.append(np.abs(np.argmax(xx) - np.argmax(yy)))\n",
    "        xx[xx<0] = 0\n",
    "        yy[yy<0] = 0\n",
    "        zz = xx*yy\n",
    "        p1s.append(peaks(zz))\n",
    "        p2s.append(areas(zz))\n",
    "    data = [\n",
    "        np.abs(df_fw['fw_rho']-df_bw['bw_rho']),\n",
    "        # np.abs(df_fw['fw_theta']-df_bw['bw_theta']),\n",
    "        [np.dot(x[1][0]/np.linalg.norm(x[1][0]), y[1][0]/np.linalg.norm(y[1][0])) for x,y in zip(data_fw, data_bw)],\n",
    "        p1s,\n",
    "        p2s,\n",
    "        pam\n",
    "    ]\n",
    "    columns = [\n",
    "        'err_rho',\n",
    "        # 'err_theta',\n",
    "        'sum_corr',\n",
    "        'sum_p1',\n",
    "        'sum_p2',\n",
    "        'peak_dist'\n",
    "    ]\n",
    "    df = pd.DataFrame(np.array(data).T, index=df_fw.index, columns=columns)\n",
    "    return df\n",
    "\n",
    "def read_dataframes(preprocess, train:bool):\n",
    "    global data\n",
    "    global data_fw\n",
    "    global data_bw\n",
    "    data = readcgws(records=records, preprocess=preprocess, train=train)\n",
    "    if train:\n",
    "        data_fw = [(x[0], y[0], x[2]) for x in data for y in x[1]]\n",
    "        data_bw = [(x[0], y[1], x[2]) for x in data for y in x[1]]\n",
    "    else:\n",
    "        data_fw = [(x[0], x[1][0], x[2]) for x in data]\n",
    "        data_bw = [(x[0], x[1][1], x[2]) for x in data]\n",
    "    df_fw, df_lbp_fw = raw2df(data_fw, 'fw', train)\n",
    "    df_bw, df_lbp_bw = raw2df(data_bw, 'bw', train)\n",
    "    df_mix = features_mixed(df_fw, df_bw, data_fw, data_bw)\n",
    "    df = pd.concat((df_fw, df_bw, df_mix), axis=1)\n",
    "    df_lbp = pd.concat((df_lbp_fw, df_lbp_bw), axis=1)\n",
    "    df.drop(['fw_theta', 'bw_theta'], axis=1, inplace=True)\n",
    "    if train:\n",
    "        targets = pd.DataFrame(np.array([x[-1] for x in data]), index=df.index.unique(level='record'), columns=['target'])\n",
    "        return df, df_lbp, targets\n",
    "    else:\n",
    "        return df, df_lbp\n",
    "\n",
    "def get_dfs(train:bool) -> tuple:\n",
    "    if train:\n",
    "        return read_cache_dfs(\n",
    "            infiles=[f'{x}_train.csv' for x in ['df', 'df_lbp', 'targets']],\n",
    "            read_function=lambda : read_dataframes(preprocess=augmented_features, train=train),\n",
    "            index_col=[0,1],\n",
    "            outfiles=[f'{x}_train.csv' for x in ['df', 'df_lbp', 'targets']]\n",
    "        )\n",
    "    else:\n",
    "        return read_cache_dfs(\n",
    "            infiles=[f'{x}_test.csv' for x in ['df', 'df_lbp']],\n",
    "            read_function=lambda : read_dataframes(preprocess=features, train=train),\n",
    "            index_col=0,\n",
    "            outfiles=[f'{x}_test.csv' for x in ['df', 'df_lbp']]\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read, augment and extract features and save the preprocessed records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = read_labels()\n",
    "records = records[records['target']!=-1].index.values\n",
    "df, df_lbp, targets = get_dfs(train=True)  # read and save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
